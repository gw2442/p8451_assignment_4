---
title: "P8451: Assignment 4"
output: html_document
date: "2023-02-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Part 0: Data Cleaning

### Loading packages and preparing dataset

To proceed with the problem set, the following libraries will be used in addition to base R
```{r}
library(tidyverse)
library(caret)
library(Amelia)
library(dplyr)
library(stats)
library(factoextra)
library(cluster)
library(ggpubr)

set.seed(123)
```

The data set is first imorted using the `read_csv` function, and is cleaned using the `clean_names` function. The dataset is then summarised using the `skim` function. 

```{r}
nyc_doh_data = read_csv(file = "data/class4_p1.csv") %>%
  janitor::clean_names()
  skimr::skim(nyc_doh_data)
```

Based off of the codebook provided, the following variables are then renamed and recoded using the `mutate` function:

* chronic1 = hypertension
* chronic3 = diabetes
* chronic4 = asthma
* tobacco1 = tobacco
* alcohol1 = alcohol
* habits5 = physical_activity_category
* habits7 = diet_category
* agegroup = age_category
* dem3 = sex
* dem4 = hispanic
* dem8 = country_origin
* povertygroup = household_annual_income
* x1 = id 
* gpaq8totmin = physical_activity_min
* gpaq11days = walk_avg_day


While all variables are imported as numeric variables, the codebook indicates that the first 12 variables listed are factor variables and are therefore recoded according to the codebook. 

All entries with NA are omitted using `na.omit`. Duplicate entries are omitted as well using the `distinct` function.

```{r}
nyc_doh_data =
  nyc_doh_data %>%
  mutate(
    id = x1,
    hypertension = factor(chronic1, labels = c("Yes","No")),
    diabetes = factor(chronic3, labels = c("Yes","No")),
    asthma = factor(chronic4, labels = c("Yes", "No")),
    tobacco = factor(tobacco1, labels = c("Most or All Days",
                                          "Some Days",
                                          "Never")),
    alcohol = factor(alcohol1, labels = c("Most or All Days",
                                          "Some Days",
                                          "Never")),
    physical_activity_min = gpaq8totmin,
    walk_avg_day = gpaq11days,
    physical_activity_category = factor(habits5, labels = c("Very Active",
                                                            "Somewhat Active",
                                                            "Not Very Active",
                                                            "Not Active At All")),
    diet_category = factor(habits7, labels = c("Excellent",
                                               "Very Good",
                                               "Good",
                                               "Fair",
                                               "Poor")),
    age_category = factor(agegroup, labels = c("18-24 Yrs", 
                                               "25-44 Yrs",
                                               "45-65 Yrs",
                                               "65+ Yrs")),
    sex = factor(dem3, labels = c("Male", "Female")),
    hispanic = factor(dem4, labels = c("Yes", "No")),
    country_origin = factor(dem8, labels = c("USA", "Outside USA")),
    household_annual_income = factor(povertygroup, labels = c("<100%",
                                                       "100-199%",
                                                       "200-399%",
                                                       "400-599%",
                                                       "600%+",
                                                       "Don't Know")),
    healthy_days = healthydays
  ) %>%
  select(id, hypertension, diabetes, asthma, bmi, tobacco, alcohol, physical_activity_min, walk_avg_day, physical_activity_category, diet_category, age_category, sex, hispanic, country_origin, household_annual_income, healthy_days) %>%
  na.omit() %>%
  distinct(id, .keep_all = TRUE)
```

The dataset `nyc_doh_data` now has 2195 observations. The dataset has 17 variables, 12 of which are factor variables (`hypertension`, `diabetes`, `asthma`, `tobacco`, `alcohol`, `physical_activty_category`, `diet_category`, `age_category`, `sex`, `hispanic`, `country_of_origin`, `household_annual_income`) and 5 of which are numeric variables (`id`, `bmi`, `physical_activity_min`, `walk_avg_day`, `healthy_days`). 

### Removing highly correlated features and centering and scaling

Prior to analysis, we want to remove highly correlated features to avoid introducing error. To do this, the numeric variables in our original dataset `nyc_doh_data` need to be selected and the `cor` function is applied to calculate correlations. Factor variables are omitted, since their correlations cannot be calculated. A cutoff point of 0.4 is used to identify highly correlated features using the function `findCorrelation`. The output is stored in the object `high_correlations`. 

```{r}
nyc_doh_numeric_data = 
  nyc_doh_data %>%
  select(where(is.numeric))

correlations = 
  cor(nyc_doh_numeric_data,
      use = "complete.obs")

high_correlations = findCorrelation(correlations, cutoff = 0.4)
```

The output indicates that there are no values in the `high_correlations` object, indicating that there are no highly correlated features that need to be removed. We can then proceed with the centering and scaling

```{r}
set.up.preprocess = 
  preProcess(nyc_doh_numeric_data, method = c("center", "scale"))

transformed.vals = 
  predict(set.up.preprocess, nyc_doh_numeric_data)
```

### Creating balanced partitions in the data 

The data is then partitioned into training and testing using a 70/30 split by using the function `createDataPartition`. The training and testing dataset is  generated with an equal proportion of individual with the outcome of interest, in this case `healthy_days`. 

```{r}
train.index = 
  createDataPartition(nyc_doh_data$healthy_days, p = 0.7, list = FALSE)

nyc_doh_train = nyc_doh_data[train.index,]
nyc_doh_test = nyc_doh_data[-train.index,]
```


## Part I: Implementing a Simple Prediction Pipline 

### 1. Fitting two prediction models using different subsets of the features in the training data 

Two models are created to predict the outcome variables `healthy_days`. The first model created, `model_1`, includes the following features:

* sex
* age_category
* hispanic 
* bmi 
* tobacco
* alcohol
* physical_activity_category
* diet_category
* physical_activity_min
* walk_avg_day

We use the `trainControl` function to set validation method and options. We perform a 10-fold cross-validation for our analysis. The output is then applied to the `train` function. To minimise the RMSE, we apply a tuning grid for lambda and alpha. 

```{r}
control.settings = trainControl(method = "cv", number = 10)

lambda = seq(0, 0.2, length = 20)
lambda_grid = expand.grid(alpha = 1, lambda = lambda)

set.seed(123)
model_1 = 
  train(healthy_days ~ sex + age_category + hispanic + bmi + tobacco + alcohol + physical_activity_category + diet_category + physical_activity_min + walk_avg_day, data = nyc_doh_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

model_1$bestTune
```

The second model created, `model_2`, includes the following features: 

* sex
* age_category
* hispanic 
* bmi 
* country_origin
* hypertension
* diabetes
* asthma

The same approach is taken with `model_2` to generate the following output:

```{r}
control.settings = trainControl(method = "cv", number = 10)

lambda = seq(0, 0.3, length = 20)
lambda_grid = expand.grid(alpha = 1, lambda = lambda)

set.seed(123)
model_2 = 
  train(healthy_days ~ sex + age_category + hispanic + bmi + country_origin + hypertension + diabetes + asthma, data = nyc_doh_train, method = "glmnet", preProc = c("center", "scale"), trControl = control.settings, tuneGrid = lambda_grid)

model_2$bestTune
```

### 2. Applying both models within the test data 

Both models are applied within the test data to determine which model is the preferred prediction model using the `predict` function. The RMSE is then calculated to determine whic model is the preferred prediction

```{r}
test_outcome_1 =
  predict(model_1, nyc_doh_test)
rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_1, nyc_doh_test$healthy_days)

test_outcome_2 = 
  predict(model_2, nyc_doh_test)
rmse = function(actual, expected) {
  residuals = actual - expected
  sqrt(mean(residuals^2))
}

rmse(test_outcome_2, nyc_doh_test$healthy_days)
```

The output indicates that `model_1` is the preferred prediction, as its RMSE value is the smaller value among the two models. 

### 3. Implementation of final model 

`model_2` contains both demographic variables and variables that relate to the patient's medical history. Conversely, `model_1` contains both demographic variables and variables that refer to the patient's self-reported habits. `model_1` would be useful in instances where only patients' demographic and self-reported habits data is available. 


## Part II: Conducting an Unsupervised Analysis 

The data set used when conducting the unsupervised analysis is the data set USArrests available in R.

### 4. Conducting a hierarchical clustering analysis 

#### Load data and prepare for analysis
```{r}
us_arrests = USArrests %>%
  janitor::clean_names() 

skimr::skim(us_arrests)
```

The data set is comprised of 50 observations and 4 variables, all of which are numeric. There are no missing variables in the data set. 

The means and standard deviations are then checked using the `colMeans` function to determine if scaling is necessary 

```{r}
colMeans(us_arrests, na.rm = TRUE)
apply(us_arrests, 2, sd, na.rm = TRUE)
```

The output indicates that scaling is necessary, as the means and standard deviations are largely variable across the data set. Therefore, the data set is centered and scaled accordingly using the `prcomp` function. 

```{r}
us_arrests_pca = 
  prcomp( ~ ., data = us_arrests, center = TRUE, scale = TRUE, na.action = na.omit)

us_arrests_pca$x
```

#### Hierarchical clustering analysis

To conduct a hierarchical clustering analysis, a dissimilarity matrix is first created using the `dist` function. A Euclidian distance measure is used to construct the dissimilarity matrix. The `hclust` function is used to employ the complete linkage method. The following dendrogram is then obtained

```{r}
diss_matrix = dist(us_arrests_pca$x, method = "euclidean")

clusters_h = hclust(diss_matrix, method = "complete" )

plot(clusters_h, cex = 0.6, hang = -1)
```

To identify the optimal number of clusters, the function `hclusCut` is created to use within `clusGap`. 

```{r}
hclusCut = function (x, k) list (cluster = cutree(hclust(dist(x, method = "euclidian"), method = "average"), k = k))

gap_stat = clusGap(us_arrests_pca$x, FUN = hclusCut, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

The following output indicates that the optimal number of clusters is 2.


### 5. Research question using the newly identified clusters

One research question that can be addressed using the newly identified clusters is which states have higher rates of arrests per 100,000 residentse for murder. The exposure in this research question are the clusters, while the outcome is the arrests per 100,000 residents for murder (`murder`). Before using these clusters for the proposed research question, it is imperative to analyse the ways in which data was obtained. If arrests are conducted in a biased manner (i.e. arrests conducted were racially biased), then the analysis would result in biased clustering, producing skewed results. 





